{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ee072f4",
   "metadata": {},
   "source": [
    "## Sentimental Analysis on Amazon product textual reviews.\n",
    "#### - By NIKITA UDAYSING PATIL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49aafe88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries \n",
    "import numpy as np\n",
    "import  pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from textblob  import TextBlob\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# pip install nltk\n",
    "import os\n",
    "import nltk \n",
    "import nltk.corpus\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords \n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_excel(r'iphone data.xlsx')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132c6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc43c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.review_country.value_counts()\n",
    "data.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b151c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=data.dropna()\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b99d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.review_rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0297f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['review_rating']=data['review_rating'].replace(['5.0 out of 5 stars'],'5')\n",
    "data1['review_rating']=data1['review_rating'].replace(['4.0 out of 5 stars'],'4')\n",
    "data1['review_rating']=data1['review_rating'].replace(['3.0 out of 5 stars'],'3')\n",
    "data1['review_rating']=data1['review_rating'].replace(['2.0 out of 5 stars'],'2')\n",
    "data1['review_rating']=data1['review_rating'].replace(['1.0 out of 5 stars'],'1')\n",
    "data1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2288435",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary statistics of numerical features : \\n\", data1.describe())\n",
    "\n",
    "print(\"\\nTotal number of reviews: \",len(data1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae052b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ed1c32",
   "metadata": {},
   "source": [
    "# Step 1: Data Visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282de104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of rating\n",
    "plt.figure(figsize=(8,8))\n",
    "# sns.countplot(df['Rating'])\n",
    "data1['review_rating'].value_counts().sort_index().plot(kind='bar',color='violet')\n",
    "plt.title('Distribution of Rating')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f0996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of text_review length\n",
    "review_length = data1[\"review_text\"].dropna().map(lambda x: len(x))\n",
    "plt.figure(figsize=(5,5))\n",
    "review_length.loc[review_length < 100].hist(color='maroon',grid=False)\n",
    "plt.title(\"Distribution of Review Length\")\n",
    "plt.xlabel('Review length (Number of character)')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of rating\n",
    "#plt.figure(figsize=(18,8))\n",
    "# sns.countplot(df['Rating'])\n",
    "#data1['reviewed_at'].value_counts().sort_index().plot(kind='bar',color='violet')\n",
    "#plt.title('Distribution of Rating')\n",
    "#plt.xlabel('Rating')\n",
    "#plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddee0ae",
   "metadata": {},
   "source": [
    "# Step 2: Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a70989",
   "metadata": {},
   "source": [
    "For illustrative purpose, to find polarity of given \"review_text\" I use TextBlob. After that I converted it into 'positive_sentiment' as (1) when \"sentiment_polarity > 0\" and 'negative_sentiment' as (0) when \" sentiment_polarity < 0 \" . Also I drop reviews contain \"sentiment_polarity == 0\" because it is referred as 'neutral'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef1ac1",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7cfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pol(review_rating):\n",
    "    return TextBlob(review_rating).sentiment.polarity\n",
    "data1['sentiment_polarity'] = data1['review_text'].apply(find_pol)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_negative = data1[data1.sentiment_polarity <0].review_text\n",
    "print(most_negative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1114cca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_positive = data1[data1.sentiment_polarity >0].review_text\n",
    "print(most_positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0897aac",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Drop missing values\n",
    "data1.dropna(inplace=True)\n",
    "\n",
    "# Remove any 'neutral' ratings equal to 0\n",
    "data2 = data1[data1['sentiment_polarity'] != 0]\n",
    "data2\n",
    "# Encode  1 as (positive sentiment) and  0  as (negative sentiment)\n",
    "data1['sentiment'] = np.where(data1['sentiment_polarity'] < 0, 0, 1)\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f441d6b7",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9507514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training set and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(data1['review_text'], data1['sentiment'], \n",
    "                                                    test_size=0.20, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5027576",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d7efc2",
   "metadata": {},
   "source": [
    "# Step 3: Bag of Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc91b0c",
   "metadata": {},
   "source": [
    "The main aim of this project is to classify reviews in textual format into positive and negative sentiment. So there are two steps needed. Firstly we find word embedding convert textual reviews in numerical representation and secondly,then fit supervised machine learning algorithms on that numerical representation.\n",
    "\n",
    "Word embedding is frequency based embedding such as Bag of Words (BoW) model. This model learns a vocubulary list from a given corpus and represent each document based on some counting methods of words.In this part,we will explore the model using BoW with SVM algorithms.\n",
    "\n",
    "The following steps of workflow:\n",
    "\n",
    "* step 1: preprocess raw text_reviews into clean one.\n",
    "* step 2: Create Bow using CountVectorizer / Tfidfvectorizer in sklearn.\n",
    "* step 3: Transform review text to numerical representations\n",
    "* step 4: Fit SVM algorithm (e.g Naive Bayes,Logistic regression, etc.)\n",
    "* step 5: Improve the model performance by GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ec4e6",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7266f63f",
   "metadata": {},
   "source": [
    "The following steps are implemented to convert raw text_reviews into clean text_reviews.\n",
    "\n",
    "* Remove Html tags using BeautifulSoup.\n",
    "* Remove non_character such as digits and symbols.\n",
    "* Convert uppercase letters into lowercase.\n",
    "* Remove stop words such as \"the\", \"and\" .....\n",
    "* Convert root word by stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3154e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False, \\\n",
    "             ):\n",
    "    '''\n",
    "    Convert a raw review to a cleaned review\n",
    "    '''\n",
    "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
    "    words = letters_only.lower().split() # convert to lower case \n",
    "    \n",
    "    if remove_stopwords: # remove stopword\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        \n",
    "    if stemming==True: # stemming\n",
    "#         stemmer = PorterStemmer()\n",
    "        stemmer = SnowballStemmer('english') \n",
    "        words = [stemmer.stem(w) for w in words]\n",
    "        \n",
    "    if split_text==True:  # split text\n",
    "        return (words)\n",
    "    \n",
    "    return( \" \".join(words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a002d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanText(raw_text=data.review_text[5], remove_stopwords=True, stemming=True, split_text=True, \\\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text data in training set and testing set\n",
    "X_train_cleaned = []\n",
    "X_test_cleaned = []\n",
    "\n",
    "for d in X_train:\n",
    "    X_train_cleaned.append(cleanText(d))\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[5])\n",
    "    \n",
    "for d in X_test:\n",
    "    X_test_cleaned.append(cleanText(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e759585",
   "metadata": {},
   "source": [
    "# CountVectorizer with Multinomial Naive Bayes \n",
    "(Benchmark Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc9bf5",
   "metadata": {},
   "source": [
    "Now our text_reviews are cleaned !! The next step is to convert them into numerical representations for SVM algorithm.\n",
    "In sklearn library , we can use CountVectorizer which implements both tokenization and counting in a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ba3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data to a document-term matrix using CountVectorizer\n",
    "countVect = CountVectorizer() \n",
    "X_train_countVect = countVect.fit_transform(X_train_cleaned)\n",
    "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) \n",
    "print(\"Show some feature names : \\n\", countVect.get_feature_names()[::100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf40e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MultinomialNB classifier\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_countVect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5134d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4957aa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countVect.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32854e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelEvaluation(predictions):\n",
    "    '''\n",
    "    Print model evaluation to predicted result \n",
    "    '''\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb2d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on validaton set\n",
    "predictions = mnb.predict(countVect.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba686277",
   "metadata": {},
   "source": [
    "# TfidfVectorizer with Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d9febc",
   "metadata": {},
   "source": [
    "\n",
    "Some words might frequently apper but have meaningful information about the sentiment of a particular review.Insted of using occurance counting,we can use tf-idf transform to scale down the impact of frequntly words in given corpus.\n",
    "\n",
    "In sklearn library,we can use TfidVectorizer which implements both tokenization and ti-idf weighted counting on a single class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1addd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the training data to a document-term matrix using TfidfVectorizer \n",
    "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) \n",
    "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751eac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "X_train_tfidf.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704bc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the top 10 features with smallest and the largest coefficients\n",
    "feature_names = np.array(tfidf.get_feature_names())\n",
    "sorted_coef_index = lr.coef_[0].argsort()\n",
    "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:100]]))\n",
    "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f32ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validaton set\n",
    "predictions = lr.predict(tfidf.transform(X_test_cleaned))\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6166c230",
   "metadata": {},
   "source": [
    "# pipeline and GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934ebc68",
   "metadata": {},
   "source": [
    "\n",
    "In sklearn library,we can build pipeline to stremline the workflow and use GridSearch on the pipline model to implememt hyper_parameter tuning for both vectorizer and classifier in one go!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a pipeline\n",
    "estimators = [(\"tfidf\", TfidfVectorizer()), (\"lr\", LogisticRegression())]\n",
    "model = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search\n",
    "params = {\"lr__C\":[0.1, 1, 10], #regularization param of logistic regression\n",
    "          \"tfidf__min_df\": [1, 5], #min count of words \n",
    "          \"tfidf__max_features\": [10, None], #max features\n",
    "          \"tfidf__ngram_range\": [(1,1), (1,2)], #1-grams or 2-grams\n",
    "          \"tfidf__stop_words\": [None, \"english\"]} #use stopwords or don't\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", n_jobs=-1)\n",
    "grid.fit(X_train_cleaned, y_train)\n",
    "print(\"The best paramenter set is : \\n\", grid.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74475644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validaton set\n",
    "predictions = grid.predict(X_test_cleaned)\n",
    "modelEvaluation(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1bd642",
   "metadata": {},
   "source": [
    "# Step 4: Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8a6b3",
   "metadata": {},
   "source": [
    "In this part,we use word cloud to get an bunch of words most appear in text_review.so I again preprocess and clean the raw text_reviews into clean ones.\n",
    "\n",
    "Here I combine all text_reviews in one variable to get easy further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=', '.join(most_negative)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc7b7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=x.lower() # lowercase letters\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "allstopwords=stopwords.words('english')\n",
    "filtered_text=text\n",
    "\n",
    "tokenized_text=word_tokenize(filtered_text)\n",
    "review_text=[word for word in tokenized_text if not word in allstopwords]\n",
    "print(review_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence=(\" \").join(review_text)\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#letters_only \n",
    "review_text= re.sub(\"[^a-zA-Z]\", \" \",filtered_sentence )\n",
    "review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a45e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud=WordCloud(background_color=\"black\").generate(review_text)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674959dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud=WordCloud(background_color=\"black\").generate(review_text)\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671898a",
   "metadata": {},
   "source": [
    "Here I found that the words like \"good\",\"phone\",\"camera\",\"battery\",\"life\",\"excellent\",\"awesome\" and so on are frequently used in most of reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3faea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DecisionTreeClassifier(criterion='entropy',max_depth=3)\n",
    "model.fit(X_train_countVect,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ebc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Decision Tree\n",
    "tree.plot_tree(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5400a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axis=plt.subplots(nrows=1,ncols=1,figsize=(3,3),dpi=200)\n",
    "tree.plot_tree(model,#feature_names=fn,class_names=cn,\n",
    "               filled=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a3690",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(countVect.transform(X_test_cleaned))\n",
    "pd.Series(predictions).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ad856",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc5e332",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243650af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross table as like the confusion matrix\n",
    "pd.crosstab(y_test,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5c110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "np.mean(predictions==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fb4915",
   "metadata": {},
   "source": [
    "Inference : After applying supervised learning algorithms we found that Naive Bayes gives model with accuracy 93.81% , logistics regression model gives 93.71% and Desicion Tree model gives accuracy 94.11% .  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
